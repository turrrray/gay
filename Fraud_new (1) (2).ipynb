{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud in Electricity and Gas Consumption #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since 2 datasets were provided, we attempt to combine both datasets into 1 on the id columm. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xander\\AppData\\Local\\Temp\\ipykernel_17568\\1767000166.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.stats import spearmanr\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import seaborn as sns\n",
    "\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "from sklearn import svm\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "seed = 69\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "invoice_df = pd.read_csv('invoice.csv')\n",
    "client_df = pd.read_csv('client.csv')\n",
    "\n",
    "combined_df = pd.merge(client_df, invoice_df, on='id', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>date_x</th>\n",
       "      <th>dis</th>\n",
       "      <th>id</th>\n",
       "      <th>catg</th>\n",
       "      <th>target</th>\n",
       "      <th>date_y</th>\n",
       "      <th>tarif_type</th>\n",
       "      <th>counter_number</th>\n",
       "      <th>counter_statue</th>\n",
       "      <th>...</th>\n",
       "      <th>reading_remarque</th>\n",
       "      <th>consommation_level_4</th>\n",
       "      <th>old_index</th>\n",
       "      <th>new_index</th>\n",
       "      <th>months_number</th>\n",
       "      <th>counter_type</th>\n",
       "      <th>counter_coefficient</th>\n",
       "      <th>consommation_level_1</th>\n",
       "      <th>consommation_level_2</th>\n",
       "      <th>consommation_level_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>31/12/1994</td>\n",
       "      <td>60</td>\n",
       "      <td>train_Client_0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>24/3/2014</td>\n",
       "      <td>11</td>\n",
       "      <td>1335667.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>14302</td>\n",
       "      <td>14384</td>\n",
       "      <td>4</td>\n",
       "      <td>ELEC</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>31/12/1994</td>\n",
       "      <td>60</td>\n",
       "      <td>train_Client_0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>29/3/2013</td>\n",
       "      <td>11</td>\n",
       "      <td>1335667.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>12294</td>\n",
       "      <td>13678</td>\n",
       "      <td>4</td>\n",
       "      <td>ELEC</td>\n",
       "      <td>1</td>\n",
       "      <td>1200</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>31/12/1994</td>\n",
       "      <td>60</td>\n",
       "      <td>train_Client_0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>23/3/2015</td>\n",
       "      <td>11</td>\n",
       "      <td>1335667.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>14624</td>\n",
       "      <td>14747</td>\n",
       "      <td>4</td>\n",
       "      <td>ELEC</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>31/12/1994</td>\n",
       "      <td>60</td>\n",
       "      <td>train_Client_0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>13/7/2015</td>\n",
       "      <td>11</td>\n",
       "      <td>1335667.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>14747</td>\n",
       "      <td>14849</td>\n",
       "      <td>4</td>\n",
       "      <td>ELEC</td>\n",
       "      <td>1</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101</td>\n",
       "      <td>31/12/1994</td>\n",
       "      <td>60</td>\n",
       "      <td>train_Client_0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>17/11/2016</td>\n",
       "      <td>11</td>\n",
       "      <td>1335667.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>15066</td>\n",
       "      <td>15638</td>\n",
       "      <td>12</td>\n",
       "      <td>ELEC</td>\n",
       "      <td>1</td>\n",
       "      <td>572</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   region      date_x  dis              id  catg  target      date_y  \\\n",
       "0     101  31/12/1994   60  train_Client_0    11       0   24/3/2014   \n",
       "1     101  31/12/1994   60  train_Client_0    11       0   29/3/2013   \n",
       "2     101  31/12/1994   60  train_Client_0    11       0   23/3/2015   \n",
       "3     101  31/12/1994   60  train_Client_0    11       0   13/7/2015   \n",
       "4     101  31/12/1994   60  train_Client_0    11       0  17/11/2016   \n",
       "\n",
       "   tarif_type  counter_number  counter_statue  ...  reading_remarque  \\\n",
       "0          11       1335667.0               0  ...                 8   \n",
       "1          11       1335667.0               0  ...                 6   \n",
       "2          11       1335667.0               0  ...                 8   \n",
       "3          11       1335667.0               0  ...                 8   \n",
       "4          11       1335667.0               0  ...                 9   \n",
       "\n",
       "   consommation_level_4  old_index  new_index  months_number  counter_type  \\\n",
       "0                     0      14302      14384              4          ELEC   \n",
       "1                     0      12294      13678              4          ELEC   \n",
       "2                     0      14624      14747              4          ELEC   \n",
       "3                     0      14747      14849              4          ELEC   \n",
       "4                     0      15066      15638             12          ELEC   \n",
       "\n",
       "  counter_coefficient  consommation_level_1  consommation_level_2  \\\n",
       "0                   1                    82                     0   \n",
       "1                   1                  1200                   184   \n",
       "2                   1                   123                     0   \n",
       "3                   1                   102                     0   \n",
       "4                   1                   572                     0   \n",
       "\n",
       "   consommation_level_3  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>dis</th>\n",
       "      <th>catg</th>\n",
       "      <th>target</th>\n",
       "      <th>tarif_type</th>\n",
       "      <th>counter_number</th>\n",
       "      <th>counter_statue</th>\n",
       "      <th>counter_code</th>\n",
       "      <th>reading_remarque</th>\n",
       "      <th>consommation_level_4</th>\n",
       "      <th>old_index</th>\n",
       "      <th>new_index</th>\n",
       "      <th>months_number</th>\n",
       "      <th>counter_coefficient</th>\n",
       "      <th>consommation_level_1</th>\n",
       "      <th>consommation_level_2</th>\n",
       "      <th>consommation_level_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500651.000000</td>\n",
       "      <td>500651.000000</td>\n",
       "      <td>500651.000000</td>\n",
       "      <td>500651.000000</td>\n",
       "      <td>500651.000000</td>\n",
       "      <td>5.006510e+05</td>\n",
       "      <td>500651.000000</td>\n",
       "      <td>500651.000000</td>\n",
       "      <td>500651.000000</td>\n",
       "      <td>500651.000000</td>\n",
       "      <td>5.006510e+05</td>\n",
       "      <td>5.006510e+05</td>\n",
       "      <td>500651.000000</td>\n",
       "      <td>500651.000000</td>\n",
       "      <td>500651.000000</td>\n",
       "      <td>500651.000000</td>\n",
       "      <td>500651.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>204.746922</td>\n",
       "      <td>63.519156</td>\n",
       "      <td>11.353871</td>\n",
       "      <td>0.062644</td>\n",
       "      <td>16.108279</td>\n",
       "      <td>1.951034e+11</td>\n",
       "      <td>0.050217</td>\n",
       "      <td>204.390755</td>\n",
       "      <td>7.463710</td>\n",
       "      <td>64.393150</td>\n",
       "      <td>1.575969e+04</td>\n",
       "      <td>1.639037e+04</td>\n",
       "      <td>22.744289</td>\n",
       "      <td>1.000154</td>\n",
       "      <td>443.065463</td>\n",
       "      <td>120.508706</td>\n",
       "      <td>28.196772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>104.620488</td>\n",
       "      <td>3.388720</td>\n",
       "      <td>3.661420</td>\n",
       "      <td>0.242323</td>\n",
       "      <td>11.145881</td>\n",
       "      <td>2.071552e+12</td>\n",
       "      <td>0.396153</td>\n",
       "      <td>121.204514</td>\n",
       "      <td>1.374409</td>\n",
       "      <td>1230.465569</td>\n",
       "      <td>2.975733e+04</td>\n",
       "      <td>3.053707e+04</td>\n",
       "      <td>1670.624818</td>\n",
       "      <td>0.047150</td>\n",
       "      <td>592.249623</td>\n",
       "      <td>1396.817086</td>\n",
       "      <td>214.020756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>101.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>101.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.477220e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.799000e+03</td>\n",
       "      <td>2.165000e+03</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>107.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>4.857010e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>203.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.876000e+03</td>\n",
       "      <td>8.438000e+03</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>307.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.008740e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.092750e+04</td>\n",
       "      <td>2.164500e+04</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>661.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>399.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>2.740000e+13</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>343568.000000</td>\n",
       "      <td>2.800280e+06</td>\n",
       "      <td>2.870972e+06</td>\n",
       "      <td>231602.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>98889.000000</td>\n",
       "      <td>819886.000000</td>\n",
       "      <td>45360.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              region            dis           catg         target  \\\n",
       "count  500651.000000  500651.000000  500651.000000  500651.000000   \n",
       "mean      204.746922      63.519156      11.353871       0.062644   \n",
       "std       104.620488       3.388720       3.661420       0.242323   \n",
       "min       101.000000      60.000000      11.000000       0.000000   \n",
       "25%       101.000000      62.000000      11.000000       0.000000   \n",
       "50%       107.000000      62.000000      11.000000       0.000000   \n",
       "75%       307.000000      69.000000      11.000000       0.000000   \n",
       "max       399.000000      69.000000      51.000000       1.000000   \n",
       "\n",
       "          tarif_type  counter_number  counter_statue   counter_code  \\\n",
       "count  500651.000000    5.006510e+05   500651.000000  500651.000000   \n",
       "mean       16.108279    1.951034e+11        0.050217     204.390755   \n",
       "std        11.145881    2.071552e+12        0.396153     121.204514   \n",
       "min         9.000000    0.000000e+00        0.000000       5.000000   \n",
       "25%        11.000000    1.477220e+05        0.000000     202.000000   \n",
       "50%        11.000000    4.857010e+05        0.000000     203.000000   \n",
       "75%        11.000000    1.008740e+06        0.000000     207.000000   \n",
       "max        45.000000    2.740000e+13        5.000000     600.000000   \n",
       "\n",
       "       reading_remarque  consommation_level_4     old_index     new_index  \\\n",
       "count     500651.000000         500651.000000  5.006510e+05  5.006510e+05   \n",
       "mean           7.463710             64.393150  1.575969e+04  1.639037e+04   \n",
       "std            1.374409           1230.465569  2.975733e+04  3.053707e+04   \n",
       "min            6.000000              0.000000  0.000000e+00  0.000000e+00   \n",
       "25%            6.000000              0.000000  1.799000e+03  2.165000e+03   \n",
       "50%            8.000000              0.000000  7.876000e+03  8.438000e+03   \n",
       "75%            9.000000              0.000000  2.092750e+04  2.164500e+04   \n",
       "max            9.000000         343568.000000  2.800280e+06  2.870972e+06   \n",
       "\n",
       "       months_number  counter_coefficient  consommation_level_1  \\\n",
       "count  500651.000000        500651.000000         500651.000000   \n",
       "mean       22.744289             1.000154            443.065463   \n",
       "std      1670.624818             0.047150            592.249623   \n",
       "min         1.000000             1.000000              0.000000   \n",
       "25%         4.000000             1.000000             99.000000   \n",
       "50%         4.000000             1.000000            321.000000   \n",
       "75%         4.000000             1.000000            661.000000   \n",
       "max    231602.000000            20.000000          98889.000000   \n",
       "\n",
       "       consommation_level_2  consommation_level_3  \n",
       "count         500651.000000         500651.000000  \n",
       "mean             120.508706             28.196772  \n",
       "std             1396.817086            214.020756  \n",
       "min                0.000000              0.000000  \n",
       "25%                0.000000              0.000000  \n",
       "50%                0.000000              0.000000  \n",
       "75%                0.000000              0.000000  \n",
       "max           819886.000000          45360.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>date_x</th>\n",
       "      <th>dis</th>\n",
       "      <th>id</th>\n",
       "      <th>catg</th>\n",
       "      <th>target</th>\n",
       "      <th>date_y</th>\n",
       "      <th>tarif_type</th>\n",
       "      <th>counter_number</th>\n",
       "      <th>counter_statue</th>\n",
       "      <th>...</th>\n",
       "      <th>reading_remarque</th>\n",
       "      <th>consommation_level_4</th>\n",
       "      <th>old_index</th>\n",
       "      <th>new_index</th>\n",
       "      <th>months_number</th>\n",
       "      <th>counter_type</th>\n",
       "      <th>counter_coefficient</th>\n",
       "      <th>consommation_level_1</th>\n",
       "      <th>consommation_level_2</th>\n",
       "      <th>consommation_level_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>31/12/1994</td>\n",
       "      <td>60</td>\n",
       "      <td>train_Client_0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>24/3/2014</td>\n",
       "      <td>11</td>\n",
       "      <td>1335667.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>14302</td>\n",
       "      <td>14384</td>\n",
       "      <td>4</td>\n",
       "      <td>ELEC</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>31/12/1994</td>\n",
       "      <td>60</td>\n",
       "      <td>train_Client_0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>29/3/2013</td>\n",
       "      <td>11</td>\n",
       "      <td>1335667.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>12294</td>\n",
       "      <td>13678</td>\n",
       "      <td>4</td>\n",
       "      <td>ELEC</td>\n",
       "      <td>1</td>\n",
       "      <td>1200</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>31/12/1994</td>\n",
       "      <td>60</td>\n",
       "      <td>train_Client_0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>23/3/2015</td>\n",
       "      <td>11</td>\n",
       "      <td>1335667.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>14624</td>\n",
       "      <td>14747</td>\n",
       "      <td>4</td>\n",
       "      <td>ELEC</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>31/12/1994</td>\n",
       "      <td>60</td>\n",
       "      <td>train_Client_0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>13/7/2015</td>\n",
       "      <td>11</td>\n",
       "      <td>1335667.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>14747</td>\n",
       "      <td>14849</td>\n",
       "      <td>4</td>\n",
       "      <td>ELEC</td>\n",
       "      <td>1</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101</td>\n",
       "      <td>31/12/1994</td>\n",
       "      <td>60</td>\n",
       "      <td>train_Client_0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>17/11/2016</td>\n",
       "      <td>11</td>\n",
       "      <td>1335667.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>15066</td>\n",
       "      <td>15638</td>\n",
       "      <td>12</td>\n",
       "      <td>ELEC</td>\n",
       "      <td>1</td>\n",
       "      <td>572</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   region      date_x  dis              id  catg  target      date_y  \\\n",
       "0     101  31/12/1994   60  train_Client_0    11       0   24/3/2014   \n",
       "1     101  31/12/1994   60  train_Client_0    11       0   29/3/2013   \n",
       "2     101  31/12/1994   60  train_Client_0    11       0   23/3/2015   \n",
       "3     101  31/12/1994   60  train_Client_0    11       0   13/7/2015   \n",
       "4     101  31/12/1994   60  train_Client_0    11       0  17/11/2016   \n",
       "\n",
       "   tarif_type  counter_number  counter_statue  ...  reading_remarque  \\\n",
       "0          11       1335667.0               0  ...                 8   \n",
       "1          11       1335667.0               0  ...                 6   \n",
       "2          11       1335667.0               0  ...                 8   \n",
       "3          11       1335667.0               0  ...                 8   \n",
       "4          11       1335667.0               0  ...                 9   \n",
       "\n",
       "   consommation_level_4  old_index  new_index  months_number  counter_type  \\\n",
       "0                     0      14302      14384              4          ELEC   \n",
       "1                     0      12294      13678              4          ELEC   \n",
       "2                     0      14624      14747              4          ELEC   \n",
       "3                     0      14747      14849              4          ELEC   \n",
       "4                     0      15066      15638             12          ELEC   \n",
       "\n",
       "  counter_coefficient  consommation_level_1  consommation_level_2  \\\n",
       "0                   1                    82                     0   \n",
       "1                   1                  1200                   184   \n",
       "2                   1                   123                     0   \n",
       "3                   1                   102                     0   \n",
       "4                   1                   572                     0   \n",
       "\n",
       "   consommation_level_3  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 500651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints in each column: \n",
      "region                  500651\n",
      "date_x                  500651\n",
      "dis                     500651\n",
      "id                      500651\n",
      "catg                    500651\n",
      "target                  500651\n",
      "date_y                  500651\n",
      "tarif_type              500651\n",
      "counter_number          500651\n",
      "counter_statue          500651\n",
      "counter_code            500651\n",
      "reading_remarque        500651\n",
      "consommation_level_4    500651\n",
      "old_index               500651\n",
      "new_index               500651\n",
      "months_number           500651\n",
      "counter_type            500651\n",
      "counter_coefficient     500651\n",
      "consommation_level_1    500651\n",
      "consommation_level_2    500651\n",
      "consommation_level_3    500651\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of dataset: {len(combined_df)}\")\n",
    "print(f\"Number of datapoints in each column: \\n{combined_df.count()} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31363\n",
      "proportion of fraud: 0.06264443694310008\n"
     ]
    }
   ],
   "source": [
    "number_of_fraud = sum(combined_df[\"target\"] == 1)\n",
    "print(number_of_fraud)\n",
    "print(f\"proportion of fraud: {number_of_fraud/len(combined_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have created 4 new variables, described as such:\n",
    "##### delta_start_invoice: diff between join and transaction date\n",
    "##### delta_index: diff between old and new index\n",
    "##### delta_transactions: diff between transactions over the same client\n",
    "##### consommation_sum: sum of consommation levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>join_date</th>\n",
       "      <th>transaction_date</th>\n",
       "      <th>id</th>\n",
       "      <th>delta_start_invoice</th>\n",
       "      <th>delta_transactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1994-12-31</td>\n",
       "      <td>2005-10-17</td>\n",
       "      <td>train_Client_0</td>\n",
       "      <td>3943</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1994-12-31</td>\n",
       "      <td>2006-02-24</td>\n",
       "      <td>train_Client_0</td>\n",
       "      <td>4073</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1994-12-31</td>\n",
       "      <td>2006-06-23</td>\n",
       "      <td>train_Client_0</td>\n",
       "      <td>4192</td>\n",
       "      <td>119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1994-12-31</td>\n",
       "      <td>2006-10-18</td>\n",
       "      <td>train_Client_0</td>\n",
       "      <td>4309</td>\n",
       "      <td>117.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1994-12-31</td>\n",
       "      <td>2007-02-26</td>\n",
       "      <td>train_Client_0</td>\n",
       "      <td>4440</td>\n",
       "      <td>131.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    join_date transaction_date              id  delta_start_invoice  \\\n",
       "22 1994-12-31       2005-10-17  train_Client_0                 3943   \n",
       "23 1994-12-31       2006-02-24  train_Client_0                 4073   \n",
       "24 1994-12-31       2006-06-23  train_Client_0                 4192   \n",
       "25 1994-12-31       2006-10-18  train_Client_0                 4309   \n",
       "28 1994-12-31       2007-02-26  train_Client_0                 4440   \n",
       "\n",
       "    delta_transactions  \n",
       "22                 0.0  \n",
       "23               130.0  \n",
       "24               119.0  \n",
       "25               117.0  \n",
       "28               131.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dates = {'join_date': combined_df['date_x'], 'transaction_date': combined_df['date_y'], 'id': combined_df['id']}\n",
    "dates_df = pd.DataFrame(dates)\n",
    "\n",
    "dates_df['join_date'] = pd.to_datetime(dates_df['join_date'] , format='%d/%m/%Y')\n",
    "dates_df['transaction_date'] = pd.to_datetime(dates_df['transaction_date'], format='%d/%m/%Y')\n",
    "\n",
    "# Calculate the difference in days between transaction and join date\n",
    "dates_df['delta_start_invoice'] = (dates_df['transaction_date']- dates_df['join_date']).dt.days\n",
    "\n",
    "# Create new delta_transactions (diff between transaction dates for each client)\n",
    "dates_df = dates_df.sort_values(['id', 'delta_start_invoice'])\n",
    "dates_df['delta_transactions'] = dates_df.groupby('id')['delta_start_invoice'].diff().fillna(0)\n",
    "\n",
    "dates_df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add new delta_start_invoice, delta_index and consommation_sum to combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['delta_index'] = combined_df['new_index'] - combined_df['old_index']\n",
    "combined_df['delta_start_invoice'] = dates_df['delta_start_invoice']\n",
    "combined_df['delta_transactions'] = dates_df['delta_transactions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### new dataframe for one-hot encoding categorical variables (dis, catg, region, tarif_type, counter_statue, counter_code, reading_remarque, counter_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_vars = ['dis', 'catg', 'region', 'tarif_type', 'counter_statue', 'counter_code', 'counter_type']\n",
    "categorical_df = pd.get_dummies(combined_df, columns=categorical_vars, prefix=categorical_vars)\n",
    "categorical_df = categorical_df.groupby('id').agg({col: 'max' for col in categorical_df.columns if col != 'id'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agg function to group the transactions with each client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">consommation_level_1</th>\n",
       "      <th colspan=\"4\" halign=\"left\">consommation_level_2</th>\n",
       "      <th colspan=\"2\" halign=\"left\">consommation_level_3</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">delta_index</th>\n",
       "      <th colspan=\"4\" halign=\"left\">delta_start_invoice</th>\n",
       "      <th colspan=\"4\" halign=\"left\">reading_remarque</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>std</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>std</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>...</th>\n",
       "      <th>median</th>\n",
       "      <th>std</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>std</th>\n",
       "      <th>sum</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_Client_0</th>\n",
       "      <td>12334</td>\n",
       "      <td>352.400000</td>\n",
       "      <td>267.0</td>\n",
       "      <td>310.343472</td>\n",
       "      <td>370</td>\n",
       "      <td>10.571429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.568935</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>267.0</td>\n",
       "      <td>341.553930</td>\n",
       "      <td>213142</td>\n",
       "      <td>6089.771429</td>\n",
       "      <td>6047.0</td>\n",
       "      <td>1358.574709</td>\n",
       "      <td>244</td>\n",
       "      <td>6.971429</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.248192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_Client_1</th>\n",
       "      <td>20629</td>\n",
       "      <td>557.540541</td>\n",
       "      <td>520.0</td>\n",
       "      <td>197.935960</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>520.0</td>\n",
       "      <td>197.935960</td>\n",
       "      <td>132603</td>\n",
       "      <td>3583.864865</td>\n",
       "      <td>3509.0</td>\n",
       "      <td>1457.748762</td>\n",
       "      <td>267</td>\n",
       "      <td>7.216216</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.377097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_Client_10</th>\n",
       "      <td>14375</td>\n",
       "      <td>798.611111</td>\n",
       "      <td>655.5</td>\n",
       "      <td>513.841374</td>\n",
       "      <td>682</td>\n",
       "      <td>37.888889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>160.748942</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>655.5</td>\n",
       "      <td>646.808386</td>\n",
       "      <td>165982</td>\n",
       "      <td>9221.222222</td>\n",
       "      <td>8678.0</td>\n",
       "      <td>1526.789733</td>\n",
       "      <td>127</td>\n",
       "      <td>7.055556</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.258955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_Client_100</th>\n",
       "      <td>24</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.607011</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.607011</td>\n",
       "      <td>91275</td>\n",
       "      <td>4563.750000</td>\n",
       "      <td>4545.5</td>\n",
       "      <td>774.520692</td>\n",
       "      <td>123</td>\n",
       "      <td>6.150000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.670820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_Client_1000</th>\n",
       "      <td>9292</td>\n",
       "      <td>663.714286</td>\n",
       "      <td>770.0</td>\n",
       "      <td>224.831365</td>\n",
       "      <td>1468</td>\n",
       "      <td>104.857143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>167.155320</td>\n",
       "      <td>1643</td>\n",
       "      <td>117.357143</td>\n",
       "      <td>...</td>\n",
       "      <td>770.0</td>\n",
       "      <td>633.485669</td>\n",
       "      <td>13497</td>\n",
       "      <td>964.071429</td>\n",
       "      <td>1010.0</td>\n",
       "      <td>506.611437</td>\n",
       "      <td>124</td>\n",
       "      <td>8.857143</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.363137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  consommation_level_1                                 \\\n",
       "                                   sum        mean median         std   \n",
       "id                                                                      \n",
       "train_Client_0                   12334  352.400000  267.0  310.343472   \n",
       "train_Client_1                   20629  557.540541  520.0  197.935960   \n",
       "train_Client_10                  14375  798.611111  655.5  513.841374   \n",
       "train_Client_100                    24    1.200000    0.0    3.607011   \n",
       "train_Client_1000                 9292  663.714286  770.0  224.831365   \n",
       "\n",
       "                  consommation_level_2                                 \\\n",
       "                                   sum        mean median         std   \n",
       "id                                                                      \n",
       "train_Client_0                     370   10.571429    0.0   43.568935   \n",
       "train_Client_1                       0    0.000000    0.0    0.000000   \n",
       "train_Client_10                    682   37.888889    0.0  160.748942   \n",
       "train_Client_100                     0    0.000000    0.0    0.000000   \n",
       "train_Client_1000                 1468  104.857143    0.0  167.155320   \n",
       "\n",
       "                  consommation_level_3              ... delta_index  \\\n",
       "                                   sum        mean  ...      median   \n",
       "id                                                  ...               \n",
       "train_Client_0                       0    0.000000  ...       267.0   \n",
       "train_Client_1                       0    0.000000  ...       520.0   \n",
       "train_Client_10                      0    0.000000  ...       655.5   \n",
       "train_Client_100                     0    0.000000  ...         0.0   \n",
       "train_Client_1000                 1643  117.357143  ...       770.0   \n",
       "\n",
       "                              delta_start_invoice                       \\\n",
       "                          std                 sum         mean  median   \n",
       "id                                                                       \n",
       "train_Client_0     341.553930              213142  6089.771429  6047.0   \n",
       "train_Client_1     197.935960              132603  3583.864865  3509.0   \n",
       "train_Client_10    646.808386              165982  9221.222222  8678.0   \n",
       "train_Client_100     3.607011               91275  4563.750000  4545.5   \n",
       "train_Client_1000  633.485669               13497   964.071429  1010.0   \n",
       "\n",
       "                               reading_remarque                             \n",
       "                           std              sum      mean median       std  \n",
       "id                                                                          \n",
       "train_Client_0     1358.574709              244  6.971429    6.0  1.248192  \n",
       "train_Client_1     1457.748762              267  7.216216    6.0  1.377097  \n",
       "train_Client_10    1526.789733              127  7.055556    6.0  1.258955  \n",
       "train_Client_100    774.520692              123  6.150000    6.0  0.670820  \n",
       "train_Client_1000   506.611437              124  8.857143    9.0  0.363137  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = ['sum', 'mean', 'median', 'std']\n",
    "\n",
    "selected_columns = ['consommation_level_1', \n",
    "                    'consommation_level_2', 'consommation_level_3', 'consommation_level_4',\n",
    "                    'delta_index', 'delta_start_invoice', 'id', 'reading_remarque']\n",
    "\n",
    "# Create a new dataframe with the desired aggregate functions\n",
    "numerical_df = combined_df[selected_columns].groupby('id').agg(stats)\n",
    "\n",
    "numerical_df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining numerical and cat dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 21652 entries, train_Client_0 to train_Client_128438\n",
      "Columns: 116 entries, ('consommation_level_1', 'sum') to counter_type_GAZ\n",
      "dtypes: bool(85), float64(22), int64(9)\n",
      "memory usage: 7.0+ MB\n"
     ]
    }
   ],
   "source": [
    "to_drop = ['region', 'date_x', 'dis', 'id', 'catg', 'target', 'date_y', 'tarif_type', 'counter_number', \n",
    "           'counter_statue', 'counter_code', 'reading_remarque', 'consommation_level_4', 'old_index',\n",
    "           'new_index', 'months_number', 'counter_type', 'counter_coefficient', 'consommation_level_1',\n",
    "           'consommation_level_2', 'consommation_level_3']\n",
    "\n",
    "client_summary = pd.concat([numerical_df, categorical_df], axis=1)\n",
    "\n",
    "# Identify existing columns in the DataFrame\n",
    "existing_columns = [col for col in to_drop if col in client_summary.columns]\n",
    "\n",
    "# Drop existing columns from the DataFrame\n",
    "client_summary = client_summary.drop(columns=existing_columns)\n",
    "\n",
    "client_summary.info()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add y variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    20576\n",
       "1     1076\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_summary['target'] = combined_df.groupby('id')['target'].apply(lambda x: 1 if x.any() else 0)\n",
    "client_summary['target'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Due to the low proportion of fraud cases, we performed synthetic oversampling of fraud cases with SMOTE and undersampled non-fraud cases with Tomek's link with three different methods:\n",
    "#### 1) SMOTE + Tomek's Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the oversampling strategy using SMOTE and Tomek\n",
    "smote = SMOTE(sampling_strategy='auto')\n",
    "\n",
    "smote = SMOTE(random_state=seed)\n",
    "tomek = TomekLinks()\n",
    "\n",
    "X = client_summary.drop('target', axis=1)\n",
    "\n",
    "# Flatten multi-level column names\n",
    "X.columns = [''.join(map(str, col)).strip() for col in X.columns.to_flat_index()]\n",
    "\n",
    "y = client_summary['target']\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.2, random_state= seed)\n",
    "\n",
    "\n",
    "# under and oversampling of data using Tomek and SMOTE\n",
    "X_train1, y_train1 = tomek.fit_resample(X_train1, y_train1)\n",
    "X_train1, y_train1 = smote.fit_resample(X_train1, y_train1)\n",
    "\n",
    "\n",
    "# Standardize the data separately to prevent leakage\n",
    "scaler = StandardScaler()\n",
    "X_train1_standardized = pd.DataFrame(scaler.fit_transform(X_train1), columns=X_train1.columns)\n",
    "X_test1_standardized = pd.DataFrame(scaler.transform(X_test1), columns=X_test1.columns)\n",
    "\n",
    "# Normalise the data\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_train1_norminalized = pd.DataFrame(scaler_minmax.fit_transform(X_train1), columns=X_train1.columns)\n",
    "X_test1_norminalized = pd.DataFrame(scaler_minmax.transform(X_test1), columns=X_test1.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) SMOTE only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Oversampling of data using SMOTE\n",
    "X_train2, y_train2 = smote.fit_resample(X_train2, y_train2)\n",
    "\n",
    "# Standardize the data separately to prevent leakage\n",
    "scaler = StandardScaler()\n",
    "X_train2_standardized = pd.DataFrame(scaler.fit_transform(X_train2), columns=X_train2.columns)\n",
    "X_test2_standardized = pd.DataFrame(scaler.transform(X_test2), columns=X_test2.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) SMOTE + ENN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ADASYN+ENN resampling strategy\n",
    "smote_enn = SMOTEENN(random_state=seed)\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Resampling of data using ADASYN+ENN\n",
    "X_train3, y_train3 = smote_enn.fit_resample(X_train3, y_train3)\n",
    "\n",
    "# Standardize the data separately to prevent leakage\n",
    "scaler = StandardScaler()\n",
    "X_train3_standardized = pd.DataFrame(scaler.fit_transform(X_train3), columns=X_train3.columns)\n",
    "X_test3_standardized = pd.DataFrame(scaler.transform(X_test3), columns=X_test3.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Bool to int\n",
    "for df in [X_train1_standardized, X_train2_standardized, X_train3_standardized,\n",
    "           X_test1_standardized, X_test2_standardized, X_test3_standardized]:\n",
    "    \n",
    "    df = df.astype({col: int for col in df.select_dtypes(include='bool').columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('np.corrcoef:', np.corrcoef(X_train1_standardized))\n",
    "cor, pval = stats.spearmanr(X_train1_standardized.T)\n",
    "print('stats.spearmanr - cor:\\n', cor)\n",
    "print('stats.spearmanr - pval\\n', pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['consommation_level_3sum', 'consommation_level_3mean', 'consommation_level_3std', 'consommation_level_2sum', 'consommation_level_2std', 'consommation_level_2mean', 'consommation_level_4sum', 'consommation_level_4std', 'consommation_level_4mean', 'delta_index', 'delta_indexsum', 'delta_indexstd', 'dis_69', 'consommation_level_1sum', 'delta_start_invoicestd', 'reading_remarquesum', 'consommation_level_1std', 'counter_statue_5', 'counter_code_203', 'counter_code_413']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bensontao/Desktop/gay/project/lib/python3.10/site-packages/pandas/core/nanops.py:1632: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return spearmanr(a, b)[0]\n"
     ]
    }
   ],
   "source": [
    "# top 20 vars with highest correlation\n",
    "corr_matrix_std = X_train1_standardized.corrwith(y_train1_resampled, method='spearman')\n",
    "\n",
    "top_20_vars = corr_matrix_std.abs().sort_values(ascending=False).head(20).index.tolist()\n",
    "\n",
    "print(top_20_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Rank                 Variable1                 Variable2  Correlation\n",
      "0      1          counter_statue_5                    dis_69     0.001869\n",
      "1      2                    dis_69          counter_statue_5     0.001869\n",
      "2      3          counter_code_413       reading_remarquesum     0.002269\n",
      "3      4       reading_remarquesum          counter_code_413     0.002269\n",
      "4      5          counter_statue_5          counter_code_413     0.007266\n",
      "5      6          counter_code_413          counter_statue_5     0.007266\n",
      "6      7    delta_start_invoicestd  consommation_level_4mean     0.014489\n",
      "7      8  consommation_level_4mean    delta_start_invoicestd     0.014489\n",
      "8      9            delta_indexsum          counter_code_203     0.014550\n",
      "9     10          counter_code_203            delta_indexsum     0.014550\n",
      "10    11                    dis_69       reading_remarquesum     0.017995\n",
      "11    12       reading_remarquesum                    dis_69     0.017995\n",
      "12    13   consommation_level_4std    delta_start_invoicestd     0.018355\n",
      "13    14    delta_start_invoicestd   consommation_level_4std     0.018355\n",
      "14    15    delta_start_invoicestd   consommation_level_4sum     0.027520\n",
      "15    16   consommation_level_4sum    delta_start_invoicestd     0.027520\n",
      "16    17          counter_code_203   consommation_level_1std     0.052838\n",
      "17    18   consommation_level_1std          counter_code_203     0.052838\n",
      "18    19  consommation_level_4mean          counter_statue_5     0.053769\n",
      "19    20          counter_statue_5  consommation_level_4mean     0.053769\n",
      "20    21          counter_statue_5   consommation_level_4std     0.056103\n",
      "21    22   consommation_level_4std          counter_statue_5     0.056103\n",
      "22    23   consommation_level_4sum          counter_statue_5     0.058655\n",
      "23    24          counter_statue_5   consommation_level_4sum     0.058655\n",
      "24    25    delta_start_invoicestd  consommation_level_3mean     0.058702\n",
      "25    26  consommation_level_3mean    delta_start_invoicestd     0.058702\n",
      "26    27    delta_start_invoicestd          counter_code_413     0.063476\n",
      "27    28          counter_code_413    delta_start_invoicestd     0.063476\n",
      "28    29  consommation_level_3mean          counter_statue_5     0.073444\n",
      "29    30          counter_statue_5  consommation_level_3mean     0.073444\n",
      "30    31    delta_start_invoicestd   consommation_level_3std     0.075808\n",
      "31    32   consommation_level_3std    delta_start_invoicestd     0.075808\n",
      "32    33   consommation_level_3std          counter_statue_5     0.081324\n",
      "33    34          counter_statue_5   consommation_level_3std     0.081324\n",
      "34    35   consommation_level_1sum                    dis_69     0.081585\n",
      "35    36                    dis_69   consommation_level_1sum     0.081585\n",
      "36    37            delta_indexstd          counter_statue_5     0.088931\n",
      "37    38          counter_statue_5            delta_indexstd     0.088931\n",
      "38    39          counter_statue_5   consommation_level_3sum     0.091687\n",
      "39    40   consommation_level_3sum          counter_statue_5     0.091687\n"
     ]
    }
   ],
   "source": [
    "# Picking 20 vars to keep (with lowest collinearity)\n",
    "\n",
    "# Create a subset of the dataset with only the top 20 variables\n",
    "X_top20 = X_train1_standardized[top_20_vars]\n",
    "\n",
    "# Remove constant columns from X_top20\n",
    "X_top20 = X_top20.loc[:, (X_top20 != X_top20.iloc[0]).any()]\n",
    "\n",
    "# Calculate correlation matrix between top 20 variables\n",
    "new_corr_matrix_std = X_top20.corr(method='spearman')\n",
    "\n",
    "# Convert the correlation matrix to a DataFrame\n",
    "corr_df = new_corr_matrix_std.reset_index().melt('index', var_name='Variable2', value_name='Correlation')\n",
    "corr_df.columns = ['Variable1', 'Variable2', 'Correlation']\n",
    "\n",
    "# Remove self-correlations (diagonal elements)\n",
    "corr_df = corr_df[corr_df['Variable1'] != corr_df['Variable2']]\n",
    "\n",
    "corr_df['Correlation'] = corr_df['Correlation'].abs()\n",
    "\n",
    "# Rank the pairs based on correlation (descending)\n",
    "corr_df = corr_df.sort_values(by='Correlation', ascending=True).reset_index(drop=True)\n",
    "\n",
    "# Add a ranking column\n",
    "corr_df.insert(0, 'Rank', range(1, len(corr_df)+1))\n",
    "\n",
    "# Print the table\n",
    "print(corr_df.head(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the correlation matrix, we pick the top 10 vars with lowest collinearity for the baseline logistic regression model\n",
    "# using method 1 (SMOTE + Tomak's Link)\n",
    "\n",
    "top_10_vars = ['counter_statue_5', 'dis_69', 'counter_code_413', 'reading_remarquesum', 'delta_start_invoicestd',\n",
    "'consommation_level_4mean', 'delta_indexsum', 'counter_code_203', 'consommation_level_4std', 'consommation_level_4sum']\n",
    "\n",
    "X_train1_standardized_resampled_important = X_train1_standardized[top_10_vars]\n",
    "X_train1_norminalized_resampled_important = X_train1_norminalized[top_10_vars]\n",
    "X_train2_standardized_important = X_train2_standardized[top_10_vars]\n",
    "X_train3_standardized_important = X_train3_standardized[top_10_vars]\n",
    "X_test_standardized_important = X_test_standardized[top_10_vars]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training std dataset shape (method 1): (32222, 10)\n",
      "Training nom dataset shape (method 1): (32222, 10)\n",
      "Training std dataset shape (method 2): (32922, 10)\n",
      "Training std dataset shape (method 3): (16971, 10)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training std dataset shape (method 1): {X_train1_standardized_resampled_important.shape}\")\n",
    "print(f\"Training nom dataset shape (method 1): {X_train1_norminalized_resampled_important.shape}\")\n",
    "print(f\"Training std dataset shape (method 2): {X_train2_standardized_important.shape}\")\n",
    "print(f\"Training std dataset shape (method 3): {X_train3_standardized_important.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation F1 Scores (method 1) (std): [0.69987021 0.75429975 0.75460123 0.7496136  0.74361342]\n",
      "Mean F1 Score (method 1) (std): 0.74039964323809\n",
      "Cross-Validation F1 Scores (method 1) (nom): [0.6913178  0.74888787 0.74911552 0.7442866  0.73904997]\n",
      "Mean F1 Score (method 1) (nom): 0.7345315513896734\n"
     ]
    }
   ],
   "source": [
    "# Comparing performance of standardized and nominalized datasets for method 1 (SMOTE + Tomak's Link)\n",
    "\n",
    "# Initialize logistic regression model\n",
    "model_LR = LogisticRegression()\n",
    "\n",
    "# Perform cross-validation (std)\n",
    "cross_val_results_std = cross_val_score(model_LR, X_train1_standardized_resampled_important, y_train1_resampled, cv=5, scoring='f1')\n",
    "\n",
    "# Print cross-validation F1 score (std)\n",
    "print(\"Cross-Validation F1 Scores (method 1) (std):\", cross_val_results_std)\n",
    "print(\"Mean F1 Score (method 1) (std):\", cross_val_results_std.mean())\n",
    "\n",
    "# Perform cross-validation (nom)\n",
    "cross_val_results_nom = cross_val_score(model_LR, X_train1_norminalized_resampled_important, y_train1_resampled, cv=5, scoring='f1')\n",
    "\n",
    "# Print cross-validation F1 score (nom)\n",
    "print(\"Cross-Validation F1 Scores (method 1) (nom):\", cross_val_results_nom)\n",
    "print(\"Mean F1 Score (method 1) (nom):\", cross_val_results_nom.mean())\n",
    "\n",
    "# We pick standardising over norminalizing given its slightly better mean F1 score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation F1 Scores (method 2) (std): [0.69572158 0.75192047 0.75045154 0.7496977  0.74861007]\n",
      "Mean F1 Score (method 2) (std): 0.7392802717960579\n",
      "Cross-Validation F1 Scores (method 3) (std): [0. 0. 0. 0. 0.]\n",
      "Mean F1 Score (method 3) (std): 0.0\n"
     ]
    }
   ],
   "source": [
    "# Now, running logistic regression on standardised datasets for method 2 (SMOTE only) and method 3 (Tomak's Link only)\n",
    "\n",
    "# Perform cross-validation (method 2)\n",
    "cross_val_results_m2 = cross_val_score(model_LR, X_train2_standardized_important, y_train2, cv=5, scoring='f1')\n",
    "\n",
    "# Print cross-validation F1 score (method 2)\n",
    "print(\"Cross-Validation F1 Scores (method 2) (std):\", cross_val_results_m2)\n",
    "print(\"Mean F1 Score (method 2) (std):\", cross_val_results_m2.mean())\n",
    "\n",
    "# Perform cross-validation (method 3)\n",
    "cross_val_results_m3 = cross_val_score(model_LR, X_train3_standardized_important, y_train3, cv=5, scoring='f1')\n",
    "\n",
    "# Print cross-validation F1 score (method 3)\n",
    "print(\"Cross-Validation F1 Scores (method 3) (std):\", cross_val_results_m3)\n",
    "print(\"Mean F1 Score (method 3) (std):\", cross_val_results_m3.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation F2 Scores: [0.79581484 0.96610581 0.97460452 0.97270246 0.9656952 ]\n",
      "Mean F2 Score: 0.9349845666002047\n"
     ]
    }
   ],
   "source": [
    "# Create an SVM classifier\n",
    "SVM = svm.SVC(kernel='linear')\n",
    "\n",
    "def f2_score(y_true, y_pred):\n",
    "    return fbeta_score(y_true, y_pred, beta=2)\n",
    "\n",
    "# Make scorer\n",
    "f2_scorer = make_scorer(f2_score)\n",
    "\n",
    "cross_val_results_m2_svm = cross_val_score(SVM, X_train2_standardized, y_train2, cv=5, scoring=f2_scorer)\n",
    "print(\"Cross-Validation F2 Scores:\", cross_val_results_m2_svm)\n",
    "print(\"Mean F2 Score:\", cross_val_results_m2_svm.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 13169, number of negative: 13168\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001276 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1542\n",
      "[LightGBM] [Info] Number of data points in the train set: 26337, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500019 -> initscore=0.000076\n",
      "[LightGBM] [Info] Start training from score 0.000076\n",
      "[LightGBM] [Info] Number of positive: 13168, number of negative: 13169\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001472 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1542\n",
      "[LightGBM] [Info] Number of data points in the train set: 26337, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499981 -> initscore=-0.000076\n",
      "[LightGBM] [Info] Start training from score -0.000076\n",
      "[LightGBM] [Info] Number of positive: 13169, number of negative: 13169\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1542\n",
      "[LightGBM] [Info] Number of data points in the train set: 26338, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 13169, number of negative: 13169\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000934 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1542\n",
      "[LightGBM] [Info] Number of data points in the train set: 26338, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 13169, number of negative: 13169\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000951 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1542\n",
      "[LightGBM] [Info] Number of data points in the train set: 26338, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Cross-Validation F2 Scores: [0.77340445 0.84288645 0.83670512 0.8334846  0.83378582]\n",
      "Mean F2 Score: 0.8240532875317648\n"
     ]
    }
   ],
   "source": [
    "# Create a LightGBM classifier\n",
    "GBM = lgb.LGBMClassifier()\n",
    "\n",
    "cross_val_results_m2_gbm = cross_val_score(GBM, X_train2_standardized_important, y_train2, cv=5, scoring=f2_scorer)\n",
    "print(\"Cross-Validation F2 Scores:\", cross_val_results_m2_gbm)\n",
    "print(\"Mean F2 Score:\", cross_val_results_m2_gbm.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F2 Score: 0.2425531914893617\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Create a Random Forest classifier\n",
    "# RF = RandomForestClassifier(n_estimators=100,random_state=seed)\n",
    "\n",
    "# cross_val_results_m2_rf = cross_val_score(RF, X_train2_standardized_important, y_train2, cv=5, scoring=f2_scorer)\n",
    "# print(\"Cross-Validation F2 Scores:\", cross_val_results_m2_rf)\n",
    "# print(\"Mean F2 Score:\", cross_val_results_m2_rf.mean())\n",
    "\n",
    "# X_train_standardized = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "\n",
    "# cross_val_results_m2_rf2 = cross_val_score(RF, X_train2_standardized, y_train2, cv=5, scoring=f2_scorer)\n",
    "# print(\"Cross-Validation F2 Scores:\", cross_val_results_m2_rf2)\n",
    "# print(\"Mean F2 Score:\", cross_val_results_m2_rf2.mean())\n",
    "\n",
    "class_weights = {0: 1, 1: 20}  # 0 for non-fraud, 1 for fraud\n",
    "\n",
    "# Initialize Random Forest classifier with class weights\n",
    "RF = RandomForestClassifier(n_estimators=100,random_state=seed,class_weight=class_weights)\n",
    "\n",
    "# Fit the model\n",
    "RF.fit(X_train4_standardized, y_train4)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = RF.predict(X_test_standardized)\n",
    "\n",
    "# Calculate F2 score\n",
    "f2_score = fbeta_score(y_test, y_pred, beta=2)\n",
    "\n",
    "# Print F2 score\n",
    "print(\"F2 Score:\", f2_score)\n",
    "#0.2744948532215021 balanced\n",
    "#0.28329297820823246 class_weights 1:20\n",
    "#0.27791814047498736 1:30\n",
    "#0.27096114519427406 1:40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20}\n",
      "Best F2 score:  0.9706735817173368\n",
      "F2 Score on Test Set: 0.5157401205626256\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define the reduced parameter grid for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 500],\n",
    "    'max_depth': [None, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 4],\n",
    "    'max_features': ['sqrt', None]\n",
    "}\n",
    "\n",
    "# Create the RandomForestClassifier\n",
    "RF = RandomForestClassifier(random_state=seed, class_weight='balanced')\n",
    "\n",
    "# Create the f2_scorer\n",
    "f2_scorer = make_scorer(fbeta_score, beta=2)\n",
    "\n",
    "# Create StratifiedKFold for cross-validation with reduced splits\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "\n",
    "# Perform RandomizedSearchCV with reduced iterations\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=RF,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    cv=skf,\n",
    "    scoring=f2_scorer,\n",
    "    random_state=seed,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV on the standardized training data\n",
    "rf_random.fit(X_train1_standardized, y_train1_resampled)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters found: \", rf_random.best_params_)\n",
    "print(\"Best F2 score: \", rf_random.best_score_)\n",
    "\n",
    "# Predict on the standardized test data using the best model\n",
    "y_pred = rf_random.best_estimator_.predict(X_test_standardized)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "f2_score = fbeta_score(y_test, y_pred, beta=2)\n",
    "print(\"F2 Score on Test Set:\", f2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'subsample': 0.8, 'scale_pos_weight': 1, 'n_estimators': 150, 'max_depth': 3, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n",
      "Best score:  0.949940491307499\n",
      "Cross-validation scores:  [0.90896665 0.97065524 0.97019958]\n",
      "Mean cross-validation score:  0.949940491307499\n",
      "F2 Score on Test Set: 0.1706827309236948\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Define the parameter grid for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.1, 0.01],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'scale_pos_weight': [1, 5]\n",
    "}\n",
    "\n",
    "# Create the XGBoost classifier\n",
    "xgb = XGBClassifier(random_state=seed, n_jobs=-1)\n",
    "\n",
    "# Perform RandomizedSearchCV with default scoring\n",
    "xgb_random = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    random_state=seed,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV on the standardized training data\n",
    "xgb_random.fit(X_train2_standardized, y_train2)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters found: \", xgb_random.best_params_)\n",
    "print(\"Best score: \", xgb_random.best_score_)\n",
    "\n",
    "# Perform cross-validation with the best model\n",
    "best_model = xgb_random.best_estimator_\n",
    "cv_scores = cross_val_score(best_model, X_train2_standardized, y_train2, cv=3, n_jobs=-1)\n",
    "print(\"Cross-validation scores: \", cv_scores)\n",
    "print(\"Mean cross-validation score: \", cv_scores.mean())\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_model.predict(X_test_standardized)\n",
    "f2_score = fbeta_score(y_test, y_pred, beta=2)\n",
    "print(\"F2 Score on Test Set:\", f2_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F2 Score: 0.2039615610904099\n"
     ]
    }
   ],
   "source": [
    "SVM.fit(X_train2_standardized, y_train2)\n",
    "\n",
    "y_pred = SVM.predict(X_test_standardized)\n",
    "f2_score = fbeta_score(y_test, y_pred, beta=2)\n",
    "print(\"F2 Score:\", f2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ADA boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bensontao/Desktop/gay/project/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/bensontao/Desktop/gay/project/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/bensontao/Desktop/gay/project/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/bensontao/Desktop/gay/project/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "/Users/bensontao/Desktop/gay/project/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation F2 Scores: [0.72985084 0.79057117 0.78921836 0.78216019 0.7920493 ]\n",
      "Mean F2 Score: 0.7767699724614421\n"
     ]
    }
   ],
   "source": [
    "ADA = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200)\n",
    "\n",
    "cross_val_results_m2_ada = cross_val_score(ADA, X_train2_standardized_important, y_train2, cv=5, scoring=f2_scorer)\n",
    "print(\"Cross-Validation F2 Scores:\", cross_val_results_m2_ada)\n",
    "print(\"Mean F2 Score:\", cross_val_results_m2_ada.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
